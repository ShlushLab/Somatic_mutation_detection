---
title: "R Notebook"
output: html_notebook
---
# Variant Preprocessing Script

## Introduction
This script processes variant data derived from genomic analyses. Below is an overview of the preprocessing steps and how this script fits into the workflow.

### Variant Calling
Variant calling was performed using Mutect2, Genome Analysis Toolkit (GATK) v.4.1.7.0. The process was conducted in 'tumor-only' mode with default parameters. The following resources were utilized:

- **Genome Aggregation Database (gnomAD):** Used as a germline resource to help filter out potential germline variants. ([PMC7334197](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7334197/))
- **Population Reference:** Derived from the 1000 Genomes Project (1000GP), serving as a reference of germline variants. ([PubMed: 26432245](https://pubmed.ncbi.nlm.nih.gov/26432245/))
- **GATK Best Practices Repository:** Resources obtained from "gs://gatk-best-practices/somatic-hg38".

### Post-Variant Calling Steps
1. **LearnReadOrientationModel:** This step learns the prior probability of read orientation artifacts.
2. **FilterMutectCalls:** Filters variants based on several criteria, incorporating the orientation model.
3. **PASS Filtering:** Only variants flagged as "PASS" were selected for downstream analyses.

### Annotation
Gene annotation was performed using ANNOVAR ([documentation](https://annovar.openbioinformatics.org/)).

## Purpose of the Script
This script takes ANNOVAR-generated files as input and performs filtering of the variants based on specific criteria. At the end of the process, the script writes the filtered variants to an output file for downstream analysis.

## Features
- Filters variants based on customizable criteria.
- Handles input files generated by ANNOVAR.
- Outputs a clean and focused dataset of variants for subsequent analyses.

## Input
- ANNOVAR-generated annotation files.

## Output
- A filtered dataset containing only the variants that meet the specified criteria.

## Workflow Overview
1. Read and parse input files.
2. Apply filtering criteria to the variant data.
3. Write the filtered variants to an output file.



```{r}
library(dplyr)
library(tidyr)
library(stringr)
library(purrr)
```

#read in vcf files after annovar
```{r}
files <- list.files(path ="/path/", pattern = "fileending.txt", full.names = T)
path<- "/path/"
```
#convert to df
```{r}


process_file <- function(file_path) {
  file_name <- basename(file_path)
  print(file_name)
  
  # Read the file into a tibble
  df <- read_delim(file_path, '\t', name_repair = "minimal") %>%
    as_tibble() 
  
  # Split Otherinfo11 into separate key-value columns
  df_split <- df %>%
    separate_rows(Otherinfo11, sep = ";") %>%
    separate(Otherinfo11, into = c("key", "value"), sep = "=", fill = "right") %>%
    pivot_wider(names_from = key, values_from = value)
  
  # Step 1: Split Otherinfo12 and Otherinfo13 into keys and values
  df_split_2 <- df_split %>%
    rowwise() %>%
    mutate(
      keys = list(str_split(Otherinfo12, ":")[[1]]),   # Split keys into list
      values = list(str_split(Otherinfo13, ":")[[1]])  # Split values into list
    ) %>%
    ungroup()

  # Step 2: Determine the number of keys in the first row
  num_keys <- length(df_split_2$keys[[1]])
  
  # Step 3: For each row, keep only the number of keys/values equal to the first row
  df_split_2 <- df_split_2 %>%
    mutate(
      keys = map(keys, ~ .x[1:num_keys]),         # Keep only the first 'num_keys' keys
      values = map(values, ~ .x[1:num_keys])      # Keep only the first 'num_keys' values
    )
  
  # Step 4: Create a tibble from the keys and values, and unnest them into separate columns
  df_final <- df_split_2 %>%
    mutate(data = map2(keys, values, ~set_names(as.list(.y), .x))) %>%
    unnest_wider(data, names_sep = "_",names_repair = "check_unique") %>%
    select(-Otherinfo12, -Otherinfo13, -keys, -values)  # Optionally remove the original columns
  
  return(df_final)
}

```


#helper function to calculate p value of strand bias
```{r}
# Define the function to compute chi-square p-value
compute_chisq_pvalue <- function(F1R2, F2R1) {
  # Convert the input strings into numeric vectors
  F1R2_values <- as.numeric(strsplit(F1R2, ",")[[1]])
  F2R1_values <- as.numeric(strsplit(F2R1, ",")[[1]])
  
  # Create a 2x2 matrix for the chi-square test
  chi_matrix <- matrix(c(F1R2_values[1], F1R2_values[2], F2R1_values[1], F2R1_values[2]), nrow = 2)
  
  
  # Check if all values in the matrix are zero
  if (all(chi_matrix == 0)) {
    return(0.9) # Return 0.9 if the matrix is invalid for chi-square testing
  }
  # Perform the chi-square test
  chisq_result <- chisq.test(chi_matrix)
  
    p_value <- chisq_result$p.value
    
    
  if (is.nan(p_value)) {
    p_value <- 0.9
  }
  
  # Return the p-value
  return(p_value)
}

```

#apply all filtering
```{r}
# Create a global list to store the filtering results
filter_results <- list()

# Function to filter data and save the number of rows after each step
filter_file <- function(df, df_name) {
  #df = df_final
  #df_name = "df1"
  
  # Initialize a named list to store filter results for this specific df
  filter_results[[df_name]] <<- list()
  
  # Step 1: Initial number of mutations
  init_rows <- dim(df)[1]
  #print(paste("initial number of mutations: ", init_rows))
  filter_results[[df_name]]$initial <<- init_rows
  
  # Step 2: Filter on "PASS"
  filtered_df <- df %>%
    filter(Otherinfo10 == "PASS")
  pass_rows <- dim(filtered_df)[1]
  #print(paste("number of PASS filter mutect calls mutations: ", pass_rows))
  filter_results[[df_name]]$pass <<- pass_rows
  
  # Step 3: MBQ difference filter
  filtered_df <- filtered_df %>%
    separate(MBQ, into = c("MBQ1", "MBQ2"), sep = ",") %>%
    mutate(
      MBQ1 = as.numeric(MBQ1),
      MBQ2 = as.numeric(MBQ2),
      abs_diff = abs(MBQ1 - MBQ2)
    ) %>%
    filter(abs_diff <= 5)
  
  mbq_rows <- dim(filtered_df)[1]
  #print(paste("number with a median base quality difference of <5: ", mbq_rows))
  filter_results[[df_name]]$mbq <<- mbq_rows
  
  # Step 4: Chi-square test filter for strand bias
  filtered_df <- filtered_df %>%
    rowwise() %>%
    mutate(p_value = compute_chisq_pvalue(data_F1R2, data_F2R1)) %>%
    ungroup() %>%
    filter(p_value >= 0.01)
  
  strand_bias_rows <- dim(filtered_df)[1]
  #print(paste("filtered mutation sites showing extreme strand bias: ", strand_bias_rows))
  filter_results[[df_name]]$strand_bias <<- strand_bias_rows
  
  # Step 5: VAF filter
  #min
  filtered_df <- filtered_df %>%
    mutate(data_AF= as.numeric(data_AF))%>%
    filter(data_AF > 0.11)
  
  vaf_rows <- dim(filtered_df)[1]
  #print(paste("filtered VAF >0.11: ", vaf_rows))
  filter_results[[df_name]]$vaf_min <<- vaf_rows
  #max
  filtered_df <- filtered_df %>%
  filter(data_AF < 0.8)
  
  vaf_rows <- dim(filtered_df)[1]
  #print(paste("filtered VAF <0.8: ", vaf_rows))
  filter_results[[df_name]]$vaf_max <<- vaf_rows
  
  # Step 6: Alternate allele read count filter
  filtered_df <- filtered_df %>%
    separate(data_AD, into = c("data_AD_ref", "data_AD_alt"), sep = ",", convert = TRUE) %>%
    filter(data_AD_alt >= 3)
  
  alt_allele_rows <- dim(filtered_df)[1]
  #print(paste("filtered <3 reads supporting the alternate allele: ", alt_allele_rows))
  filter_results[[df_name]]$alt_allele <<- alt_allele_rows
  
  # Step 7: Depth filter
  filtered_df <- filtered_df %>%
    mutate(data_DP = as.numeric(data_DP))%>%
    filter(data_DP < 150)   #filter depth higher than 150
  
  depth_rows <- dim(filtered_df)[1]
  #print(paste("filtered depth >150: ", depth_rows))
  filter_results[[df_name]]$depth <<- depth_rows
  
  # Step 8: GnomAD filter
  filtered_df <- filtered_df %>%
    mutate(
      gnomad40_genome_AF_grpmax = if_else(gnomad40_genome_AF_grpmax == ".", NA_character_, gnomad40_genome_AF_grpmax) %>%
        as.numeric()
    ) %>%
    filter(is.na(gnomad40_genome_AF_grpmax) | gnomad40_genome_AF_grpmax < 0.0004467) %>%
    mutate(
      gnomad40_genome_AF_grpmax = if_else(is.na(gnomad40_genome_AF_grpmax), ".", as.character(gnomad40_genome_AF_grpmax))
    )

  
  gnomad_rows <- dim(filtered_df)[1]
  #print(paste("gnomad <3.35: ", gnomad_rows))
  filter_results[[df_name]]$gnomad <<- gnomad_rows
  
  #step 9:remove_close_variants
  filtered_df <-  remove_close_variants(filtered_df)
  bp_50<- dim(filtered_df)[1]
  #print(paste("variants within 50 bp of one another ", bp_50))
  filter_results[[df_name]]$bp_50 <<- bp_50
  
  #step10:TLOD
  filtered_df <-  filtered_df%>%
    mutate(TLOD = as.numeric(TLOD))%>%
    filter(TLOD> 40)
  tlod<- dim(filtered_df)[1]
  #print(paste("TLOD ", tlod))
  filter_results[[df_name]]$tlod <<- tlod
  
  
  return(filtered_df)  # Return the final filtered dataframe
}

```
#function to run once to determine depth cutoff and TLOD
```{r}
filter_results <- list()
depth <- list()
tlod <- list()
# Define process_files to run both process_file and filter_file on each file
process_files <- function(file_paths) {
  lapply(file_paths, function(file_path) {
    print(file_path)
    # First, process the file
    df <- process_file(file_path)
   
    depth[[basename(file_path)]] <<- df$data_DP
    tlod[[basename(file_path)]] <<-df$TLOD
      
  })
}

process_files(files)

numeric_depth <- lapply(depth, as.numeric)
numeric_tlod <- lapply(tlod, as.numeric)

# Step 2: Group lists based on names
tcga_depth <- numeric_depth[grep("^TCGA", names(numeric_depth))]
nahar_gal_depth <- numeric_depth[grep("^(Naher|Gal)", names(numeric_depth))]
non_b_depth <- numeric_depth[grep("^T\\d", names(numeric_depth))]

# Step 3: Combine lists in each group
combined_tcga <- unlist(tcga_depth, use.names = FALSE)
combined_nahar_gal <- unlist(nahar_gal_depth, use.names = FALSE)
combined_nonB <- unlist(non_b_depth, use.names = FALSE)

# Step 4: Plot distributions for each group
# Create a combined data frame with group labels
combined_data <- data.frame(
  values = c(combined_tcga, combined_nahar_gal, combined_nonB),
  group = rep(c("TCGA", "mbd4", "nonB"), 
              times = c(length(combined_tcga), length(combined_nahar_gal), length(combined_nonB)))
)



ggplot(combined_data, aes(x = group, y = values, color = group)) +
   geom_boxplot(outliers = FALSE, show.legend = FALSE)+
  #geom_jitter(show.legend = FALSE, size = 3, alpha = 0.5)+
  labs(title = "Distributions by Group", x = "Batch", y = "Depth")

```

# Define process_files to run both process_file and filter_file on each file
```{r}
filter_results <- list()

# Define process_files to run both process_file and filter_file on each file
process_files <- function(file_paths) {
  lapply(file_paths, function(file_path) {
    print(file_path)
    # First, process the file
    df <- process_file(file_path)
   
        # Then, apply filter_file to the processed dataframe
    filtered_df <- filter_file(df,  basename(file_path))
    
    # Return the final filtered dataframe
    return(filtered_df)
  })
}
files_prossesed <- process_files(files)

```
#plot vaf distribution
```{r}

# Combine the data frames, adding an identifier for each source data frame
combined_df <- bind_rows(files_samples1_prossesed, .id = "df_name") %>%
   mutate(data_AF = as.numeric(data_AF))# %>%
  # filter(data_AF < 0.7)


# Plot the distribution of VAF values for each data frame
ggplot(combined_df, aes(x = data_AF, fill = df_name)) +
  geom_density(alpha = 0.5) +  # Use density plot to visualize distribution
  labs(title = "Distribution of VAF Values Across samples",
       x = "data_AF", y = "Density") +
  scale_x_continuous(breaks = seq(0, 1, by = 0.1)) +  # Set specific x-axis breaks
  theme_minimal() +
   theme(
    legend.position = "right",
    axis.text.x = element_text(angle = 90, hjust = 1)  # Rotate x-axis labels by 90 degrees
  )



ggplot(combined_df, aes(x = data_AF, fill = df_name)) +
  geom_histogram(binwidth = 0.05, position = "dodge") +  # Histogram with bin width of 0.05
  labs(title = "Distribution of VAF Values (< 0.5) Across Data Frames",
       x = "data_AF", y = "Count") +
  scale_x_continuous(breaks = seq(0.1, 0.5, by = 0.1), limits = c(0.1, 0.5)) +  # Set x-axis range and breaks
  theme_minimal() +
  theme(legend.position = "right")


```

#plot filtering
```{r}
library(ggplot2)
library(tidyr)
library(dplyr)

# Convert filter_results into a data frame
convert_filter_results_to_df <- function(filter_results) {
  # Create an empty list to hold data for conversion
  df_list <- list()
  
  # Loop over each dataset in filter_results
  for (df_name in names(filter_results)) {
    # Convert each sublist to a dataframe and store the df_name as a column
    df_list[[df_name]] <- as.data.frame(filter_results[[df_name]]) %>%
      mutate(filter_step = rownames(.), df_name = df_name)
  }
  
  # Combine all dataframes into one
  filter_results_df <- bind_rows(df_list)
  return(filter_results_df)
}

# Example usage: convert filter_results into a data frame
filter_results_df <- convert_filter_results_to_df(filter_results)
filter_results_df_l <- pivot_longer(filter_results_df, -c(df_name, filter_step), names_to = "filter__step", values_to = "count")
filter_results_df_l$filter__step <- factor(filter_results_df_l$filter__step,
                                           levels = c("initial", "pass", "mbq", "strand_bias", "vaf", 
                                                      "alt_allele", "depth", "gnomad","bp_50"))
# Plot the grouped bar chart using ggplot2
p1 <- ggplot(filter_results_df_l, aes(x = filter__step, y = count, fill = df_name)) +
  geom_bar(stat = "identity", position = "dodge") +  # "dodge" to place bars side by side
  labs(title = "Grouped Bar Plot of Filtering Steps for Each Dataset",
       x = "Filter Step", y = "Number of Mutations") +
  theme_minimal() +
  theme(legend.position = "none")+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for readability

readr::write_delim(filter_results_df,paste0(path,"filtering_steps",".tsv"))
save_plot(paste0(path,"filtering_steps",".png"),p1)
p1

```
```{r}
# Plot with faceting by df_name
ggplot(filter_results_df_l, aes(x = filter__step, y = count, fill = filter__step)) +
  geom_bar(stat = "identity") +  # Each bar will be in its own panel
  labs(title = "#varients after Filtering Steps for Each Dataset",
       x = "Filter Step", y = "Number of Mutations") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +  # Rotate x-axis labels for readability
  facet_wrap(~ df_name, ncol = 4)  # Creates separate plots per dataset, with one column of plots
```
#find variants within df
```{r}
remove_duplicates_across_dfs <- function(df) {

  
  # Find duplicates based on the two ID columns across data frames
  duplicate_rows <- df %>%
    group_by( !!sym("Chr"), !!sym("Start"), !!sym("End")) %>%
    filter(n() > 1) %>%
    ungroup()
  
  # Print out the rows that are duplicates
  if (nrow(duplicate_rows) > 0) {
    print("Duplicate rows found and removed:")
    print(duplicate_rows)
  } else {
    print("No duplicates found across data frames.")
  }
  
  # Remove rows from original data frames that appear in duplicate_rows
 # cleaned_df_list <- lapply(df_list, function(df) {
#    anti_join(df, duplicate_rows, by = c(id_col1, id_col2))
#  })
  
 # return(cleaned_df_list)
}

repeated_elements_in_df <-remove_duplicates_across_dfs(files)
```




#find variants that appear twice and filter them out
```{r}
remove_duplicates_across_dfs <- function(df_list) {
  df_list <- files_samples1_prossesed
  # Combine all data frames into one for duplicate detection
  combined_df <- bind_rows(df_list, .id = "df_name") 
  
  # Find duplicates based on the two ID columns across data frames
  duplicate_rows <- combined_df %>%
    group_by(!!sym("df_name"), !!sym("Chr"), !!sym("Start"), !!sym("End")) %>%
    filter(n() > 1) %>%
    ungroup()
  
  # Print out the rows that are duplicates
  if (nrow(duplicate_rows) > 0) {
    print("Duplicate rows found and removed:")
    print(duplicate_rows)
  } else {
    print("No duplicates found across data frames.")
  }
  
  # Remove rows from original data frames that appear in duplicate_rows
 # cleaned_df_list <- lapply(df_list, function(df) {
#    anti_join(df, duplicate_rows, by = c(id_col1, id_col2))
#  })
  
 # return(cleaned_df_list)
}

cleaned_df_list <-remove_duplicates_across_dfs(files_prossesed)

```
#filter >50% of variant >0.15
```{r}
check_repeated_elements_with_AF <- function(df) {
  # Group by keys and identify repeated rows within the individual dataframe
  repeated_rows <- df %>%
    group_by(Chr, Start, End) %>%
    filter(n() > 1) %>%  # Keep only groups with repeats
    mutate(af_above_threshold = data_AF > 0.15) %>%  # Check if data_AF > 0.15
    summarise(
      total_repeats = n(),
      af_above_15_count = sum(af_above_threshold, na.rm = TRUE)
    ) %>%
    ungroup() %>%
    # Filter for rows where >50% of repeats have data_AF > 0.15
    filter(af_above_15_count > (total_repeats / 2))

  # Print the results
  if (nrow(repeated_rows) > 0) {
    print("Repeated rows found with data_AF > 0.15 in more than 50% of repeats:")
    print(repeated_rows)
  } else {
    print("No repeated rows with data_AF > 0.15 in more than 50% of repeats.")
  }
  
  return(repeated_rows)
}

repeated_elements_results <- check_repeated_elements_with_AF(df_final)
# Example usage with a list of data frames
repeated_elements_results <- lapply(files_samples1_prossesed, check_repeated_elements_with_AF)

```

#variants within 50 bp of one another
```{r}
remove_close_variants <- function(df) {
  # Arrange data frame by chromosome and start position
  df <- df %>% arrange(Chr, Start)

  # Identify pairs of rows with the same chromosome and less than 50 bp apart
  close_variants <- df %>%
    group_by(Chr) %>%
    mutate(
      next_start = lead(Start),
      prev_end = lag(End),
      close_to_next = (next_start - End) < 50,   # Next variant within 50 bp
      close_to_prev = (Start - prev_end) < 50    # Previous variant within 50 bp
    ) %>%
    filter(close_to_next | close_to_prev) %>%
    ungroup()

  # Count and print the number of rows that will be removed
  num_removed <- nrow(close_variants)
  print(paste(num_removed, "variants were removed due to proximity (<50 bp)"))

  # Remove close variants from the original data frame
  cleaned_df <- df %>%
    anti_join(close_variants, by = c("Chr", "Start", "End"))

  return(cleaned_df)
}

# Example usage
cleaned_df <- remove_close_variants(df_final)


```




#write to file

```{r}

path = "/output_path"

# Function to transform dataframe to VCF format
transform_to_vcf <- function(df) {
  df %>%
    mutate(ID = ".") %>%
    mutate(QUAL = ".") %>%
    mutate(FILTER = ".") %>%
    mutate(INFO = ".") %>%
    dplyr::select("Chr", "ID", "Start", "Ref", "Alt", "QUAL", "FILTER", "INFO") %>%
    dplyr::rename("#CHROM" = Chr, POS = Start, "REF" = Ref, "ALT" = Alt) %>%
    dplyr::select("#CHROM", "POS", "ID", "REF", "ALT", "QUAL", "FILTER", "INFO")
}

# Save both TSV and VCF versions
purrr::walk2(
 files_processed,
  files,
  ~{
    # Save original as TSV
    readr::write_tsv(
      .x,
      file.path(path, 
                gsub("\\.hg38_multianno\\.txt$", ".tsv", basename(.y))),
      na = "."
    )
    
    # Transform and save as VCF
    transform_to_vcf(.x) %>%
      readr::write_tsv(
        file.path(path, 
                  gsub("\\.hg38_multianno\\.txt$", ".vcf", basename(.y))),
        na = "."
      )
  }
)
```

